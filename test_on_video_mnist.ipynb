{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0+cu121'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "import inspect\n",
    "import torch.version\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 1, 30, 28, 28]),\n",
       " torch.Size([10000]),\n",
       " torch.Size([2000, 1, 30, 28, 28]),\n",
       " torch.Size([2000]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_TIMESTEPS = 30\n",
    "DATA_DIR = './data/VideoMNIST'\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "def shuffle(X:Tensor, y:Tensor): # (m, ...), (m, ...)\n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    \"\"\"returns X, y (but shuffled)\"\"\"\n",
    "    idx = torch.randperm(X.shape[0])\n",
    "    return X[idx], y[idx]\n",
    "\n",
    "class VideoMNIST:\n",
    "    def __init__(self, train:bool):\n",
    "        mnist_dataset = torchvision.datasets.MNIST(root='./data', train=train, download=True)\n",
    "        self.imgs = mnist_dataset.data/255.0\n",
    "        self.labels = mnist_dataset.targets\n",
    "        self.train = \"train\" if train else \"val\"\n",
    "\n",
    "    def create_mnist_videos_as_tensors(self, num_videos:int, target_class:int, video_length:int):\n",
    "        class_images = self.imgs[self.labels == target_class].tolist()\n",
    "        \n",
    "        if len(class_images) < video_length:\n",
    "            raise ValueError(\"Not enough images in the dataset to create the videos\")\n",
    "        \n",
    "        videos_tensor = []\n",
    "        \n",
    "        for _ in range(num_videos):\n",
    "            video_frames = [random.choice(class_images) for _ in range(video_length)]\n",
    "            video_tensor = np.array(video_frames).squeeze()[..., None] # (T, H, W, C=1)\n",
    "            videos_tensor.append(video_tensor.tolist())\n",
    "        assert len(videos_tensor) == num_videos\n",
    "        return videos_tensor, [target_class for _ in range(num_videos)] # (B_per_class, T, H, W, C=1), (B_per_class,)\n",
    "    \n",
    "    def get_dataset(self, num_videos_per_class, video_length):\n",
    "        xls, yls = [], []\n",
    "        for i in range(10):\n",
    "            x, y = self.create_mnist_videos_as_tensors(num_videos_per_class, i, video_length)\n",
    "            xls.extend(x)\n",
    "            yls.extend(y)\n",
    "        \n",
    "        images, labels = np.array(xls), np.array(yls)\n",
    "        return images, labels\n",
    "    \n",
    "    def save(self, dir_path:str, num_videos_per_class:int, video_length:int):\n",
    "        imgs, lbl = self.get_dataset(num_videos_per_class, video_length)\n",
    "        np.save(os.path.join(dir_path, f\"videos_{self.train}.npy\"), imgs)\n",
    "        np.save(os.path.join(dir_path, f\"labels_{self.train}.npy\"), lbl)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_all(dir_path:str, channels_first:bool=True):\n",
    "        train_imgs = np.load(os.path.join(dir_path, f\"videos_train.npy\")) # (B, T, H, W, C=1)\n",
    "        train_lbl = np.load(os.path.join(dir_path, f\"labels_train.npy\"))\n",
    "\n",
    "        val_imgs = np.load(os.path.join(dir_path, f\"videos_val.npy\")) # (B, T, H, W, C=1)\n",
    "        val_lbl = np.load(os.path.join(dir_path, f\"labels_val.npy\"))\n",
    "\n",
    "        if channels_first:\n",
    "            train_imgs = train_imgs.transpose(0, 4, 1, 2, 3) # (B, C=1, T, H, W)\n",
    "            val_imgs = val_imgs.transpose(0, 4, 1, 2, 3)     # (B, C=1, T, H, W)\n",
    "\n",
    "        from torch import from_numpy\n",
    "        train, val = (from_numpy(train_imgs), from_numpy(train_lbl)), (from_numpy(val_imgs), from_numpy(val_lbl))\n",
    "        (X_train, y_train), (X_val, y_val) = shuffle(*train), shuffle(*val)\n",
    "        return (X_train.float(), y_train), (X_val.float(), y_val)\n",
    "\n",
    "# Example usage:\n",
    "if len(os.listdir(DATA_DIR)) == 0:\n",
    "    VideoMNIST(train=True).save(DATA_DIR, num_videos_per_class=10_000//10, video_length=NUM_TIMESTEPS)\n",
    "    VideoMNIST(train=False).save(DATA_DIR, num_videos_per_class=2_000//10, video_length=NUM_TIMESTEPS)\n",
    "(X_train, y_train), (X_val, y_val) = VideoMNIST.load_all(DATA_DIR, channels_first=True)\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_video(video:Tensor, lbl:Tensor, channels_first:bool=True): # (C, T, H, W)\n",
    "    if channels_first:\n",
    "        video = video.permute(1, 2, 3, 0)\n",
    "    for i in range(video.shape[0]):\n",
    "        plt.imshow(video[i,..., 0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"{lbl}\")\n",
    "        plt.show()\n",
    "        clear_output(wait=True)\n",
    "        time.sleep(0.1)\n",
    "\n",
    "# play_video(X_train[25], y_train[25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViViT\n",
    "```python\n",
    "in VIT:\n",
    "    (B, H, W, C) -> (B, H//P, W//P, C*(P**2)) -> (B, N = H//P * W//P, d_model)\n",
    "\n",
    "in ViVit:\n",
    "    (B, T, H, W, C) -> (B, T//P, H//P, W//P, C*(P**3)) -> (B, N = T//P * H//P * W//P, d_model)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class config:\n",
    "    in_channels:int = 1\n",
    "    patch_size:int = 2\n",
    "    H:int = 28\n",
    "    W:int = 28\n",
    "    T:int = NUM_TIMESTEPS\n",
    "    N:int = H//patch_size * W//patch_size * T//patch_size # 28//2 * 28//2 * 30//2 = 2940\n",
    "    assert N*patch_size**3 == H*W*T, f\"{N}*{patch_size}**{3} != {H}*{W}*{T} => {N*patch_size**3} != {H*W*T}\"\n",
    "\n",
    "\n",
    "    d_model:int = 128\n",
    "    num_heads:int = 8\n",
    "    assert d_model % 2 == 0\n",
    "    assert d_model % num_heads == 0\n",
    "    dropout_rate:float = 0.0\n",
    "    num_layers:int = 5\n",
    "    maxlen:int = N+1\n",
    "\n",
    "    num_classes:int = 10\n",
    "    batch_size:int = 32\n",
    "    num_steps:int = 2000\n",
    "    log_interval:int = 1\n",
    "\n",
    "    weight_decay:float = 1e-2\n",
    "    max_learning_rate:float = 1e-3\n",
    "    min_learning_rate:float = 5e-4\n",
    "    betas:tuple[int, int] = (0.9, 0.999)\n",
    "    clipnorm:float = 1.0\n",
    "    warmup_steps:int = 100\n",
    "    eval_steps:int = 100\n",
    "    eval_freq:int = 500\n",
    "\n",
    "    dtype_type:str = 'bfloat16'\n",
    "    device:torch.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TubeletEmbed(nn.Module):\n",
    "    \"\"\"Get patch and embed it to a fixed sized vector at once\"\"\"\n",
    "    def __init__(self, config:config):\n",
    "        super().__init__()\n",
    "        self.get_patch_and_project = nn.Conv3d(\n",
    "            in_channels=config.in_channels,\n",
    "            out_channels=config.d_model,\n",
    "            kernel_size=config.patch_size,\n",
    "            stride=config.patch_size, # P\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x:Tensor): # (B, C, T, H, W)\n",
    "        \"\"\"```\n",
    "        inputs        : (B, C_in, T_in, H_in, W_in)\n",
    "        output of conv: (B, C_out, T_out = T_in//P, H_out=H_in//P, W_out=W_in//P)\n",
    "        reshape to    : (B, N = T_in//P * H_in//P * W_in//P, d_model = C_out)\n",
    "        ```\"\"\"\n",
    "        x = self.get_patch_and_project(x).flatten(2) # (B, d_model, N = T//P * H//P * W//P)\n",
    "        return x.permute(0, 2, 1) # (B, N, d_model)\n",
    "    \n",
    "emb = TubeletEmbed(config)(X_train[:2]) # (B=2, N, d_model)\n",
    "assert emb.shape == (2, config.N, config.d_model), emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatioTemporalAttentionBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config:config,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.causal = False\n",
    "        dff_in = int(4*config.d_model)\n",
    "        self.norm1 = nn.LayerNorm(config.d_model, eps=1e-5)\n",
    "        self.mha = nn.MultiheadAttention(\n",
    "            embed_dim=config.d_model,\n",
    "            num_heads=config.num_heads,\n",
    "            batch_first=True,\n",
    "            dropout=config.dropout_rate\n",
    "        )\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config.d_model, dff_in),\n",
    "            nn.GELU(approximate='tanh'),\n",
    "            nn.Linear(dff_in, config.d_model),\n",
    "            nn.Dropout(config.dropout_rate)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(config.d_model, eps=1e-5)\n",
    "    \n",
    "    def forward(self, x:Tensor):\n",
    "        in_mha = self.norm1(x)\n",
    "        z = x + self.mha(in_mha, in_mha, in_mha, is_causal=self.causal, need_weights=False)[0]\n",
    "        y = z + self.ffn(self.norm2(z))\n",
    "        return y # (B, T, d_model)\n",
    "    \n",
    "blck = SpatioTemporalAttentionBlock(config=config)(emb) # (B=2, N, d_model)\n",
    "assert blck.shape == (2, config.N, config.d_model), blck.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "class FactorizedDotProductAttentionBlock(nn.Module):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViViT(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        block:nn.Module, \n",
    "        config:config\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.tubelet_embed = TubeletEmbed(config)\n",
    "        self.pos_embeddings = nn.Parameter(\n",
    "            nn.Embedding(\n",
    "                num_embeddings=config.maxlen, # N + 1\n",
    "                embedding_dim=config.d_model\n",
    "            )(torch.arange(config.maxlen)).unsqueeze(0)\n",
    "        ) # (B=1, N, d_model)\n",
    "        \n",
    "        self.class_weight = nn.Parameter(\n",
    "            nn.Linear(1, config.d_model, bias=False).weight.T\n",
    "        ) # (1, d_model)\n",
    "        self.blocks = nn.ModuleList([deepcopy(block) for _ in range(config.num_layers)])\n",
    "\n",
    "        self.norm = nn.LayerNorm(config.d_model, eps=1e-5)\n",
    "        self.mlp_head = nn.Linear(config.d_model, config.num_classes)\n",
    "\n",
    "    def forward(self, x:Tensor): # (B, T, H, W, C)\n",
    "        x = self.tubelet_embed(x) # (B, N, d_model)\n",
    "        class_weight = self.class_weight.expand(x.shape[0], -1, -1)\n",
    "        x = torch.concat([class_weight, x], dim=1) # (B, 1+N, d_model)\n",
    "        x += self.pos_embeddings\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x) # (B, 1+N, d_model)\n",
    "\n",
    "        x = self.norm(x[:, 0, :]) # (B, d_model)\n",
    "        x = self.mlp_head(x) # (B, num_classes)\n",
    "        return x\n",
    "    \n",
    "    def configure_optimizers(\n",
    "        self,\n",
    "        weight_decay:float,\n",
    "        learning_rate:float,\n",
    "        betas:tuple[float, float],\n",
    "        device_type:str\n",
    "    ):\n",
    "        params_dict = {pname:p for pname, p in self.named_parameters() if p.requires_grad}\n",
    "\n",
    "        # all weights except layernorms and biases, embeddings and linears\n",
    "        decay_params = [p for pname, p in params_dict.items() if p.dim() >= 2]\n",
    "        # layernorms and biases\n",
    "        non_decay_params = [p for pname, p in params_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {\"params\": decay_params, \"weight_decay\": weight_decay},\n",
    "            {\"params\": non_decay_params, \"weight_decay\": 0.}\n",
    "        ]\n",
    "\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == \"cuda\"\n",
    "        # other_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            params=optim_groups,\n",
    "            lr=learning_rate,\n",
    "            betas=betas,\n",
    "            fused=False # getting error with fused=True\n",
    "        )\n",
    "        return optimizer\n",
    "    \n",
    "mod = ViViT(\n",
    "    block=SpatioTemporalAttentionBlock(config), # stab\n",
    "    config=config\n",
    ").to(config.device)\n",
    "\n",
    "mod(X_train[:2].to(config.device)) # (B=2, num_classes)\n",
    "del mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineDecayWithWarmup:\n",
    "    def __init__(\n",
    "        self,\n",
    "        warmup_steps:int,\n",
    "        max_learning_rate:float,\n",
    "        decay_steps:int,\n",
    "        min_learning_rate:float\n",
    "    ):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.max_learning_rate = max_learning_rate\n",
    "        self.decay_steps = decay_steps\n",
    "        self.min_learning_rate = min_learning_rate\n",
    "\n",
    "    def __call__(self, step):\n",
    "        # linear warmup for warmup_steps steps\n",
    "        if step < self.warmup_steps:\n",
    "            return self.max_learning_rate * step / self.warmup_steps\n",
    "        # if it > decay_steps, return min learning rate\n",
    "        if step > self.decay_steps:\n",
    "            return self.min_learning_rate\n",
    "        # in between, use cosine decay down to min learning rate\n",
    "        decay_ratio = (step - self.warmup_steps) / (self.decay_steps - self.warmup_steps)\n",
    "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "        return self.min_learning_rate + coeff * (self.max_learning_rate - self.min_learning_rate)\n",
    "    \n",
    "class DataLoader:\n",
    "    def __init__(self, dataset:tuple[Tensor, Tensor]):\n",
    "        self.X, self.y = dataset\n",
    "\n",
    "    def iter_batches(self, batch_size:int):\n",
    "        while True:\n",
    "            self.dataset = torch.utils.data.DataLoader(\n",
    "                dataset=torch.utils.data.TensorDataset(self.X, self.y),\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                pin_memory=True,\n",
    "                drop_last=True\n",
    "            )\n",
    "            for X_batch, y_batch in self.dataset:\n",
    "                yield X_batch.to(config.device), y_batch.to(config.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.370506 Million Parameters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_iterator = iter(DataLoader((X_train, y_train)).iter_batches(config.batch_size))\n",
    "val_iterator = iter(DataLoader((X_val, y_val)).iter_batches(config.batch_size))\n",
    "\n",
    "get_lr = CosineDecayWithWarmup(\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    max_learning_rate=config.max_learning_rate,\n",
    "    decay_steps=config.num_steps,\n",
    "    min_learning_rate=config.min_learning_rate\n",
    ")\n",
    "\n",
    "stab_model = ViViT(\n",
    "    block=SpatioTemporalAttentionBlock(config), # stab\n",
    "    config=config\n",
    ").to(config.device)\n",
    "stab_model.compile()\n",
    "\n",
    "optimizer = stab_model.configure_optimizers(\n",
    "    weight_decay=config.weight_decay,\n",
    "    learning_rate=config.max_learning_rate,\n",
    "    betas=config.betas,\n",
    "    device_type=config.device.type\n",
    ")\n",
    "\n",
    "ctx = torch.autocast(\n",
    "        device_type=config.device.type,\n",
    "        dtype={\"bfloat16\": torch.bfloat16,\n",
    "               \"float32\" : torch.float32}[config.dtype_type]\n",
    "    )\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model:nn.Module):\n",
    "    model.eval()\n",
    "    mean_losses, mean_metrics = [], []\n",
    "    for iterator in [train_iterator, val_iterator]:\n",
    "        losses = torch.empty((config.eval_steps,))\n",
    "        metrics = torch.empty_like(losses)\n",
    "        for i in trange(config.eval_steps):\n",
    "            X_batch, y_batch = next(iterator)\n",
    "            with ctx:\n",
    "                y_pred = model(X_batch)\n",
    "            loss = F.cross_entropy(y_pred, y_batch)\n",
    "            losses[i] = loss.item()\n",
    "            metrics[i] = (y_pred.argmax(1) == y_batch).float().mean()\n",
    "        mean_losses.append(losses.mean())\n",
    "        mean_metrics.append(metrics.mean())\n",
    "    model.train()\n",
    "\n",
    "    return mean_losses, mean_metrics\n",
    "\n",
    "print(sum(p.numel() for p in stab_model.parameters() if p.requires_grad)/1e6, \"Million Parameters\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model:nn.Module):\n",
    "    t0 = time.time()\n",
    "    losses, accuracies = [], []\n",
    "    for step in range(0, config.num_steps):\n",
    "        lr = get_lr(step)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "\n",
    "        if (step % config.eval_freq == 0 and step > 0) or step == config.num_steps-1:\n",
    "            mean_losses, mean_accuracies = evaluate(model)\n",
    "            print(\n",
    "                    f\"\\t| Training Loss: {mean_losses[0]:.4f} || Training Accuracy: {mean_accuracies[0]:.4f} |\" \n",
    "                    f\"| Validation Loss: {mean_losses[1]:.4f} || Validation Accuracy: {mean_accuracies[1]:.4f} |\"\n",
    "                )\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        X_batch, y_batch = next(train_iterator)\n",
    "        with ctx:\n",
    "            y_pred = model(X_batch) # (B, num_classes)\n",
    "        loss = F.cross_entropy(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        if config.clipnorm is not None:\n",
    "            norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=config.clipnorm, error_if_nonfinite=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        t1 = time.time()\n",
    "        dt = t1-t0\n",
    "        t0 = t1\n",
    "        if step % config.log_interval == 0:\n",
    "            lossf, acc = loss.item(), (y_pred.argmax(1) == y_batch).float().mean().item()\n",
    "            print(\n",
    "                f\"| Step: {step} || Loss: {lossf:.4f} || Accuracy: {acc:.4f} |\"\n",
    "                f\"| LR: {lr:e} || dt: {dt*1000:.2f}ms |\"\n",
    "                f\"| Norm: {norm:.4f} |\" if config.clipnorm is not None else \"\"\n",
    "            )\n",
    "            losses.append(lossf); accuracies.append(acc)\n",
    "    return losses, accuracies\n",
    "\n",
    "losses, accuracies = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Training Loss: 0.0186 || Training Accuracy: 0.9944 || Validation Loss: 0.0281 || Validation Accuracy: 0.9919 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(idx:int):\n",
    "    stab_model.eval()\n",
    "    X_batch, y_batch = X_val[idx:idx+1], y_val[idx:idx+1]\n",
    "    with torch.no_grad():\n",
    "        y_pred = stab_model(X_batch.to(config.device))\n",
    "    pred, true = y_pred.argmax(1).item(), y_batch.item()\n",
    "    print(f\"| True: {true} | Prediction: {pred} |\")\n",
    "    time.sleep(0.5)\n",
    "    play_video(X_batch[0], y_batch[0])\n",
    "    if pred != true:\n",
    "        print(\"Model made a mistake\".upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    show(random.randint(0, len(X_val)-1))\n",
    "# `play_video` clears the output, any better way to play video (ipython display doesnt work for me)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
